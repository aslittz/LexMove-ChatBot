# ================================================================
# setup_database.py â€” HUGGING FACE DATASET VERSÄ°YONU (TAM TEST EDÄ°LMÄ°Å)
# ================================================================
import os
import shutil
from pathlib import Path
from typing import List

print("ğŸ“¦ ModÃ¼ller yÃ¼kleniyor...")

try:
    import pandas as pd
    from datasets import load_dataset
except ImportError as e:
    print(f"âŒ HATA: {e}")
    print("ğŸ’¡ Åu komutu Ã§alÄ±ÅŸtÄ±rÄ±n: pip install pandas datasets")
    exit(1)

try:
    from langchain_community.vectorstores import Chroma
except ImportError:
    try:
        from langchain.vectorstores import Chroma
    except ImportError as e:
        print(f"âŒ HATA: {e}")
        print("ğŸ’¡ Åu komutu Ã§alÄ±ÅŸtÄ±rÄ±n: pip install langchain langchain-community")
        exit(1)

try:
    from langchain_huggingface import HuggingFaceEmbeddings
except ImportError:
    try:
        from langchain.embeddings import HuggingFaceEmbeddings
    except ImportError as e:
        print(f"âŒ HATA: {e}")
        print("ğŸ’¡ Åu komutu Ã§alÄ±ÅŸtÄ±rÄ±n: pip install langchain-huggingface sentence-transformers")
        exit(1)

try:
    from langchain_core.documents import Document
except ImportError:
    try:
        from langchain.schema import Document
    except ImportError as e:
        print(f"âŒ HATA: {e}")
        print("ğŸ’¡ Åu komutu Ã§alÄ±ÅŸtÄ±rÄ±n: pip install langchain-core")
        exit(1)

print("âœ… TÃ¼m modÃ¼ller yÃ¼klendi\n")

# ğŸ¯ PROJENÄ°N KÃ–K DÄ°ZÄ°NÄ°
BASE_DIR = Path(__file__).parent.absolute()

# âœ… ChromaDB'yi proje iÃ§inde tut
CHROMA_PATH = BASE_DIR / "chroma_db_lexmove_mini"

# ğŸ”¥ Hugging Face Dataset AyarlarÄ±
DATASET_NAME = "Renicames/turkish-law-chatbot"
SPLIT_NAME = "train"

COLLECTION_NAME = "mevzuat_chunks_mini"
EMBEDDING_MODEL = "all-MiniLM-L6-v2"

print("="*70)
print("ğŸš€ LexMove - ChromaDB Kurulum BaÅŸlatÄ±ldÄ± (Hugging Face Dataset)")
print("="*70)
print(f"ğŸ“¦ Dataset: {DATASET_NAME}")
print(f"ğŸ“‚ Ã‡Ä±kÄ±ÅŸ dizini (Chroma): {CHROMA_PATH}")
print(f"ğŸ“‚ Proje kÃ¶k dizini: {BASE_DIR}")

# ================================================================
# 1. HUGGING FACE'TEN VERÄ° YÃœKLEME
# ================================================================

def load_and_create_documents() -> List[Document]:
    """
    Hugging Face Q&A veri setini yÃ¼kler ve her CevabÄ±, Soru metadatasÄ±yla 
    LangChain Document listesine dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r.
    """
    all_documents = []
    
    print(f"\nâ³ Hugging Face'ten veri seti yÃ¼kleniyor: {DATASET_NAME} ({SPLIT_NAME})")
    
    try:
        # Hugging Face'ten doÄŸrudan yÃ¼kleme
        dataset = load_dataset(DATASET_NAME, split=SPLIT_NAME)
        df = dataset.to_pandas()
        
        print(f"âœ… Veri seti yÃ¼klendi. Toplam satÄ±r: {len(df)}")
        print(f"ğŸ“Š SÃ¼tunlar: {list(df.columns)}")
        
    except Exception as e:
        print(f"âŒ Veri seti yÃ¼klenirken HATA oluÅŸtu: {e}")
        print("ğŸ’¡ Ä°nternet baÄŸlantÄ±nÄ±zÄ± kontrol edin veya dataset adÄ±nÄ± doÄŸrulayÄ±n.")
        return []
    
    # Her satÄ±rÄ± LangChain Document'e dÃ¶nÃ¼ÅŸtÃ¼r
    for idx, row in df.iterrows():
        # SÃ¼tun isimlerini kontrol et (kÃ¼Ã§Ã¼k/bÃ¼yÃ¼k harf duyarlÄ±)
        question = ""
        answer = ""
        
        # FarklÄ± olasÄ± sÃ¼tun isimlerini dene
        for q_col in ['question', 'Question', 'Soru', 'soru']:
            if q_col in df.columns:
                question = str(row.get(q_col, '')).strip()
                break
        
        for a_col in ['answer', 'Answer', 'Cevap', 'cevap']:
            if a_col in df.columns:
                answer = str(row.get(a_col, '')).strip()
                break
        
        # Sadece boÅŸ olmayan cevaplarÄ± ekle
        if answer and len(answer) > 10:
            doc = Document(
                page_content=answer,
                metadata={
                    "question": question,
                    "source": DATASET_NAME,
                    "row_id": int(idx)
                }
            )
            all_documents.append(doc)
    
    print(f"âœ… {len(all_documents)} adet Document oluÅŸturuldu")
    return all_documents

# ================================================================
# 2. ESKÄ° VERÄ°TABANINI SÄ°LME
# ================================================================

if CHROMA_PATH.exists():
    print(f"\nâš ï¸ Eski veritabanÄ± siliniyor: {CHROMA_PATH}")
    try:
        shutil.rmtree(CHROMA_PATH)
        print("âœ… Eski veritabanÄ± silindi")
    except OSError as e:
        print(f"âŒ KlasÃ¶r silinirken hata: {e}")
        print("ğŸ’¡ KlasÃ¶rÃ¼ manuel olarak silmeyi deneyin veya yÃ¶netici olarak Ã§alÄ±ÅŸtÄ±rÄ±n")
        exit(1)

# ================================================================
# 3. BELGELERÄ° YÃœKLEME
# ================================================================

print("\nğŸ”¥ Belgeler Hugging Face'ten yÃ¼kleniyor...")
documents = load_and_create_documents()

if len(documents) == 0:
    print("\nâŒ HATA: YÃ¼klenecek belge bulunamadÄ±!")
    print("ğŸ’¡ Dataset boÅŸ olabilir veya sÃ¼tun isimleri farklÄ± olabilir.")
    exit(1)

print(f"\nğŸ“„ Ä°lk belge Ã¶rneÄŸi:")
print(f"  Soru: {documents[0].metadata.get('question', 'N/A')[:100]}...")
print(f"  Cevap: {documents[0].page_content[:150]}...")

# ================================================================
# 4. EMBEDDING MODELÄ°
# ================================================================

print("\nğŸ”§ Embedding modeli hazÄ±rlanÄ±yor...")
try:
    embeddings = HuggingFaceEmbeddings(
        model_name=EMBEDDING_MODEL,
        model_kwargs={"device": "cpu"},
        encode_kwargs={"normalize_embeddings": True}
    )
    print("âœ… Embedding modeli hazÄ±r")
except Exception as e:
    print(f"âŒ Embedding modeli yÃ¼klenemedi: {e}")
    print("ğŸ’¡ sentence-transformers paketi kurulu mu kontrol edin")
    print("   Komut: pip install sentence-transformers")
    exit(1)

# ================================================================
# 5. CHROMADB OLUÅTURMA
# ================================================================

print("\nğŸ§  Chroma vektÃ¶r veritabanÄ± oluÅŸturuluyor...")
print("â³ Bu iÅŸlem birkaÃ§ dakika sÃ¼rebilir...")

try:
    # ChromaDB dizinini oluÅŸtur
    CHROMA_PATH.mkdir(parents=True, exist_ok=True)
    
    # Batch processing ile vektÃ¶r oluÅŸtur (bellek tasarrufu iÃ§in)
    batch_size = 100
    total_docs = len(documents)
    
    vectorstore = None
    
    for i in range(0, total_docs, batch_size):
        batch = documents[i:i+batch_size]
        print(f"â³ Ä°ÅŸleniyor: {i+1}-{min(i+batch_size, total_docs)}/{total_docs}")
        
        if vectorstore is None:
            # Ä°lk batch ile vectorstore oluÅŸtur
            vectorstore = Chroma.from_documents(
                batch, 
                embeddings,
                persist_directory=str(CHROMA_PATH), 
                collection_name=COLLECTION_NAME
            )
        else:
            # Sonraki batch'leri ekle
            vectorstore.add_documents(batch)
    
    # VektÃ¶r sayÄ±sÄ±nÄ± kontrol et
    total_chunks = vectorstore._collection.count()
    
    print("\n" + "="*70)
    print("âœ… BAÅARILI! ChromaDB oluÅŸturuldu")
    print("="*70)
    print(f"ğŸ“Š Toplam vektÃ¶r sayÄ±sÄ±: {total_chunks}")
    print(f"ğŸ“ KayÄ±t yeri: {CHROMA_PATH}")
    print(f"ğŸ“¦ Koleksiyon adÄ±: {COLLECTION_NAME}")
    print(f"ğŸŒ Dataset: {DATASET_NAME}")
    print(f"ğŸ” Embedding Model: {EMBEDDING_MODEL}")
    print("\nğŸš€ Åimdi ÅŸu komutu Ã§alÄ±ÅŸtÄ±rÄ±n:")
    print("   streamlit run app.py")
    print("="*70)
    
except Exception as e:
    print(f"\nâŒ HATA: ChromaDB oluÅŸturulamadÄ±!")
    print(f"Detay: {e}")
    print("\nğŸ’¡ OlasÄ± Ã§Ã¶zÃ¼mler:")
    print("  1. KlasÃ¶r izinlerini kontrol edin")
    print("  2. Yeterli disk alanÄ± olduÄŸundan emin olun")
    print("  3. chromadb paketini kurun: pip install chromadb")
    import traceback
    traceback.print_exc()
    exit(1)