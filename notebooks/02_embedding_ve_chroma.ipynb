{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e574dd9-fe3f-41a1-8d1a-41d3d3380ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… API AnahtarÄ± yÃ¼klendi.\n",
      "âœ… Embedding Modeli yÃ¼klendi.\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# ğŸ“¦ KOD BLOÄU 1: KÃ¼tÃ¼phaneler, Ayarlar ve BaÄŸlantÄ± (Hugging Face Uyumlu)\n",
    "# ================================================================\n",
    "\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "import pandas as pd\n",
    "from datasets import load_dataset # Hugging Face datasets kÃ¼tÃ¼phanesi gerekli olabilir\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import google.generativeai as genai\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# --- HUGGING FACE AYARLARI ---\n",
    "DATASET_NAME = \"Renicames/turkish-law-chatbot\"\n",
    "SPLIT_NAME = \"train\" \n",
    "# Veri seti eÄŸer zaten chunk'lanmÄ±ÅŸsa, CHUNK_SIZE/OVERLAP devre dÄ±ÅŸÄ± kalÄ±r.\n",
    "# EÄŸer parÃ§alama gerekiyorsa, bunu KOD BLOK 2'de ayarlarÄ±z.\n",
    "\n",
    "# --- CHROMA DB AYARLARI (Mini-Dataset yolu aynÄ± kalÄ±r) ---\n",
    "CHROMA_PATH = os.path.expanduser(\"~/chroma_db_lexmove_mini\") \n",
    "COLLECTION_NAME = \"mevzuat_chunks_mini\"\n",
    "EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "# --- LLM VE API KEY SETUP ---\n",
    "load_dotenv(find_dotenv(usecwd=True))\n",
    "api_key = os.getenv('GOOGLE_API_KEY')\n",
    "if api_key:\n",
    "    genai.configure(api_key=api_key)\n",
    "    print(\"âœ… API AnahtarÄ± yÃ¼klendi.\")\n",
    "\n",
    "# Embedding Model\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=EMBEDDING_MODEL,\n",
    "    model_kwargs={'device': 'cpu'},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "print(\"âœ… Embedding Modeli yÃ¼klendi.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19b5af16-7514-4bd8-a695-2cf45fc3b8c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â³ Hugging Face'ten veri seti yÃ¼kleniyor: Renicames/turkish-law-chatbot (train)\n",
      "âœ… Veri seti yÃ¼klendi. Toplam satÄ±r (Document adayÄ±): 13354\n",
      "\n",
      "ğŸ“š TOPLAM 13170 Document nesnesi hazÄ±rlandÄ±.\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# ğŸ“¦ KOD BLOÄU 2: Hugging Face Q&A Veri YÃ¼kleme ve Document OluÅŸturma \n",
    "# ================================================================\n",
    "\n",
    "# Veri setindeki GERÃ‡EK sÃ¼tun isimleri\n",
    "SORU_COL = 'Soru' \n",
    "CEVAP_COL = 'Cevap' \n",
    "\n",
    "# Not: Bu veri seti zaten Q&A formatÄ±nda olduÄŸu iÃ§in Text Splitter kullanmÄ±yoruz.\n",
    "\n",
    "def load_and_create_documents() -> List[Document]:\n",
    "    \"\"\"\n",
    "    Hugging Face Q&A veri setini yÃ¼kler ve her CevabÄ±, Soru metadatasÄ± ile \n",
    "    LangChain Document listesine dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r.\n",
    "    \"\"\"\n",
    "    all_documents = []\n",
    "    \n",
    "    print(f\"â³ Hugging Face'ten veri seti yÃ¼kleniyor: {DATASET_NAME} ({SPLIT_NAME})\")\n",
    "    \n",
    "    try:\n",
    "        # Hugging Face'ten doÄŸrudan pandas DataFrame olarak yÃ¼kleme\n",
    "        dataset = load_dataset(DATASET_NAME, split=SPLIT_NAME)\n",
    "        df = dataset.to_pandas()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Veri seti yÃ¼klenirken HATA oluÅŸtu: {e}\")\n",
    "        return []\n",
    "        \n",
    "    print(f\"âœ… Veri seti yÃ¼klendi. Toplam satÄ±r (Document adayÄ±): {len(df)}\")\n",
    "    \n",
    "    # DataFrame satÄ±rlarÄ±nÄ± LangChain Document'a dÃ¶nÃ¼ÅŸtÃ¼rme\n",
    "    for index, row in df.iterrows():\n",
    "        try:\n",
    "            # SÃ¼tun isimlerini kullanarak verileri Ã§ek\n",
    "            question = str(row[SORU_COL]).strip()\n",
    "            answer = str(row[CEVAP_COL]).strip()\n",
    "            \n",
    "            # Ä°Ã§erik olarak CEVAP'Ä± kullanÄ±yoruz\n",
    "            page_content = answer \n",
    "            \n",
    "            # Soru veya Cevap Ã§ok kÄ±sa ise atla\n",
    "            if not page_content or len(page_content) < 20:\n",
    "                continue\n",
    "                \n",
    "            metadata = {\n",
    "                \"source\": DATASET_NAME,         \n",
    "                \"question\": question,           \n",
    "                \"kanun_adi\": \"Ã‡EÅÄ°TLÄ° MEVZUAT\", \n",
    "                \"madde_no\": f\"Soru-{index}\"     \n",
    "            }\n",
    "            \n",
    "            document = Document(\n",
    "                page_content=page_content,\n",
    "                metadata=metadata\n",
    "            )\n",
    "            all_documents.append(document)\n",
    "            \n",
    "        except KeyError:\n",
    "            print(f\"âš ï¸ Hata: Veri setinde '{SORU_COL}' veya '{CEVAP_COL}' sÃ¼tunlarÄ± bulunamadÄ±. LÃ¼tfen sÃ¼tun isimlerini kontrol edin.\")\n",
    "            return []\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ SatÄ±r {index} iÅŸlenirken hata: {e}\")\n",
    "            continue\n",
    "            \n",
    "    print(f\"\\nğŸ“š TOPLAM {len(all_documents)} Document nesnesi hazÄ±rlandÄ±.\")\n",
    "    return all_documents\n",
    "\n",
    "# TÃ¼m veriyi yÃ¼kle ve Document listesini oluÅŸtur\n",
    "documents = load_and_create_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "abfbf971-655c-4ae1-b63f-dab3844ae8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â³ VeritabanÄ± oluÅŸturuluyor ve 13170 parÃ§a yazÄ±lÄ±yor...\n",
      "âœ… ChromaDB baÅŸarÄ±yla oluÅŸturuldu/gÃ¼ncellendi.\n",
      "ğŸ“Š Toplam parÃ§a sayÄ±sÄ±: 13170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yt/j0nqv5qs2yv0qzc800s3hgfh0000gn/T/ipykernel_27371/4059546402.py:27: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  db.persist()\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# ğŸ“¦ KOD BLOÄU 3: ChromaDB OluÅŸturma/GÃ¼ncelleme\n",
    "# ================================================================\n",
    "\n",
    "def create_or_update_chromadb(docs: List[Document]):\n",
    "    \"\"\"Yeni verileri ChromaDB'ye yazar ve eski veriyi siler.\"\"\"\n",
    "    \n",
    "    # Eski ChromaDB'yi silme (Temiz bir baÅŸlangÄ±Ã§ iÃ§in ÅŸiddetle tavsiye edilir)\n",
    "    if Path(CHROMA_PATH).exists():\n",
    "        print(f\"âš ï¸ Eski veritabanÄ± siliniyor: {CHROMA_PATH}\")\n",
    "        shutil.rmtree(CHROMA_PATH)\n",
    "    \n",
    "    if not docs:\n",
    "        print(\"âŒ VeritabanÄ±na yazÄ±lacak dokÃ¼man yok. Ä°ÅŸlem iptal edildi.\")\n",
    "        return None\n",
    "\n",
    "    print(f\"â³ VeritabanÄ± oluÅŸturuluyor ve {len(docs)} parÃ§a yazÄ±lÄ±yor...\")\n",
    "    \n",
    "    # ChromaDB'ye yazma\n",
    "    db = Chroma.from_documents(\n",
    "        docs, \n",
    "        embeddings, \n",
    "        persist_directory=CHROMA_PATH, \n",
    "        collection_name=COLLECTION_NAME\n",
    "    )\n",
    "    \n",
    "    db.persist()\n",
    "    print(\"âœ… ChromaDB baÅŸarÄ±yla oluÅŸturuldu/gÃ¼ncellendi.\")\n",
    "    print(f\"ğŸ“Š Toplam parÃ§a sayÄ±sÄ±: {db._collection.count()}\")\n",
    "    return db\n",
    "\n",
    "# ChromaDB'yi oluÅŸtur\n",
    "mini_vectorstore = create_or_update_chromadb(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b76bf8-5603-4ff9-b691-cb5e4705305b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
