{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e574dd9-fe3f-41a1-8d1a-41d3d3380ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ API Anahtarı yüklendi.\n",
      "✅ Embedding Modeli yüklendi.\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 📦 KOD BLOĞU 1: Kütüphaneler, Ayarlar ve Bağlantı (Hugging Face Uyumlu)\n",
    "# ================================================================\n",
    "\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "import pandas as pd\n",
    "from datasets import load_dataset # Hugging Face datasets kütüphanesi gerekli olabilir\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import google.generativeai as genai\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# --- HUGGING FACE AYARLARI ---\n",
    "DATASET_NAME = \"Renicames/turkish-law-chatbot\"\n",
    "SPLIT_NAME = \"train\" \n",
    "# Veri seti eğer zaten chunk'lanmışsa, CHUNK_SIZE/OVERLAP devre dışı kalır.\n",
    "# Eğer parçalama gerekiyorsa, bunu KOD BLOK 2'de ayarlarız.\n",
    "\n",
    "# --- CHROMA DB AYARLARI (Mini-Dataset yolu aynı kalır) ---\n",
    "CHROMA_PATH = os.path.expanduser(\"~/chroma_db_lexmove_mini\") \n",
    "COLLECTION_NAME = \"mevzuat_chunks_mini\"\n",
    "EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "# --- LLM VE API KEY SETUP ---\n",
    "load_dotenv(find_dotenv(usecwd=True))\n",
    "api_key = os.getenv('GOOGLE_API_KEY')\n",
    "if api_key:\n",
    "    genai.configure(api_key=api_key)\n",
    "    print(\"✅ API Anahtarı yüklendi.\")\n",
    "\n",
    "# Embedding Model\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=EMBEDDING_MODEL,\n",
    "    model_kwargs={'device': 'cpu'},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "print(\"✅ Embedding Modeli yüklendi.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19b5af16-7514-4bd8-a695-2cf45fc3b8c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Hugging Face'ten veri seti yükleniyor: Renicames/turkish-law-chatbot (train)\n",
      "✅ Veri seti yüklendi. Toplam satır (Document adayı): 13354\n",
      "\n",
      "📚 TOPLAM 13170 Document nesnesi hazırlandı.\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 📦 KOD BLOĞU 2: Hugging Face Q&A Veri Yükleme ve Document Oluşturma \n",
    "# ================================================================\n",
    "\n",
    "# Veri setindeki GERÇEK sütun isimleri\n",
    "SORU_COL = 'Soru' \n",
    "CEVAP_COL = 'Cevap' \n",
    "\n",
    "# Not: Bu veri seti zaten Q&A formatında olduğu için Text Splitter kullanmıyoruz.\n",
    "\n",
    "def load_and_create_documents() -> List[Document]:\n",
    "    \"\"\"\n",
    "    Hugging Face Q&A veri setini yükler ve her Cevabı, Soru metadatası ile \n",
    "    LangChain Document listesine dönüştürür.\n",
    "    \"\"\"\n",
    "    all_documents = []\n",
    "    \n",
    "    print(f\"⏳ Hugging Face'ten veri seti yükleniyor: {DATASET_NAME} ({SPLIT_NAME})\")\n",
    "    \n",
    "    try:\n",
    "        # Hugging Face'ten doğrudan pandas DataFrame olarak yükleme\n",
    "        dataset = load_dataset(DATASET_NAME, split=SPLIT_NAME)\n",
    "        df = dataset.to_pandas()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Veri seti yüklenirken HATA oluştu: {e}\")\n",
    "        return []\n",
    "        \n",
    "    print(f\"✅ Veri seti yüklendi. Toplam satır (Document adayı): {len(df)}\")\n",
    "    \n",
    "    # DataFrame satırlarını LangChain Document'a dönüştürme\n",
    "    for index, row in df.iterrows():\n",
    "        try:\n",
    "            # Sütun isimlerini kullanarak verileri çek\n",
    "            question = str(row[SORU_COL]).strip()\n",
    "            answer = str(row[CEVAP_COL]).strip()\n",
    "            \n",
    "            # İçerik olarak CEVAP'ı kullanıyoruz\n",
    "            page_content = answer \n",
    "            \n",
    "            # Soru veya Cevap çok kısa ise atla\n",
    "            if not page_content or len(page_content) < 20:\n",
    "                continue\n",
    "                \n",
    "            metadata = {\n",
    "                \"source\": DATASET_NAME,         \n",
    "                \"question\": question,           \n",
    "                \"kanun_adi\": \"ÇEŞİTLİ MEVZUAT\", \n",
    "                \"madde_no\": f\"Soru-{index}\"     \n",
    "            }\n",
    "            \n",
    "            document = Document(\n",
    "                page_content=page_content,\n",
    "                metadata=metadata\n",
    "            )\n",
    "            all_documents.append(document)\n",
    "            \n",
    "        except KeyError:\n",
    "            print(f\"⚠️ Hata: Veri setinde '{SORU_COL}' veya '{CEVAP_COL}' sütunları bulunamadı. Lütfen sütun isimlerini kontrol edin.\")\n",
    "            return []\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Satır {index} işlenirken hata: {e}\")\n",
    "            continue\n",
    "            \n",
    "    print(f\"\\n📚 TOPLAM {len(all_documents)} Document nesnesi hazırlandı.\")\n",
    "    return all_documents\n",
    "\n",
    "# Tüm veriyi yükle ve Document listesini oluştur\n",
    "documents = load_and_create_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "abfbf971-655c-4ae1-b63f-dab3844ae8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Veritabanı oluşturuluyor ve 13170 parça yazılıyor...\n",
      "✅ ChromaDB başarıyla oluşturuldu/güncellendi.\n",
      "📊 Toplam parça sayısı: 13170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yt/j0nqv5qs2yv0qzc800s3hgfh0000gn/T/ipykernel_27371/4059546402.py:27: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  db.persist()\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 📦 KOD BLOĞU 3: ChromaDB Oluşturma/Güncelleme\n",
    "# ================================================================\n",
    "\n",
    "def create_or_update_chromadb(docs: List[Document]):\n",
    "    \"\"\"Yeni verileri ChromaDB'ye yazar ve eski veriyi siler.\"\"\"\n",
    "    \n",
    "    # Eski ChromaDB'yi silme (Temiz bir başlangıç için şiddetle tavsiye edilir)\n",
    "    if Path(CHROMA_PATH).exists():\n",
    "        print(f\"⚠️ Eski veritabanı siliniyor: {CHROMA_PATH}\")\n",
    "        shutil.rmtree(CHROMA_PATH)\n",
    "    \n",
    "    if not docs:\n",
    "        print(\"❌ Veritabanına yazılacak doküman yok. İşlem iptal edildi.\")\n",
    "        return None\n",
    "\n",
    "    print(f\"⏳ Veritabanı oluşturuluyor ve {len(docs)} parça yazılıyor...\")\n",
    "    \n",
    "    # ChromaDB'ye yazma\n",
    "    db = Chroma.from_documents(\n",
    "        docs, \n",
    "        embeddings, \n",
    "        persist_directory=CHROMA_PATH, \n",
    "        collection_name=COLLECTION_NAME\n",
    "    )\n",
    "    \n",
    "    db.persist()\n",
    "    print(\"✅ ChromaDB başarıyla oluşturuldu/güncellendi.\")\n",
    "    print(f\"📊 Toplam parça sayısı: {db._collection.count()}\")\n",
    "    return db\n",
    "\n",
    "# ChromaDB'yi oluştur\n",
    "mini_vectorstore = create_or_update_chromadb(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b76bf8-5603-4ff9-b691-cb5e4705305b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
